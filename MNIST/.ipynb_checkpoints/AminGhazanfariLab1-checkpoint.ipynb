{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Networks for Handwritten Digit Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will learn how to use feedforward neural networks to solve a classical task in machine learning: handwritten digit recognition using images from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). More concretely, you will have to solve the following tasks:\n",
    "\n",
    "1. implement a deep feedforward network that reads a batch of images and predicts the corresponding digits\n",
    "2. train this network using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run the code for this assignment, you need a working installation of [NumPy](http://www.numpy.org). Check whether everything works by running the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is split into a training set with 60,000 instances and a test set with 10,000 instances. Each instance consists of a greyscale image of a handwritten digit and an integer representing the digit in the image, as labelled by human experts. The digits are scaled and centred on a 28-by-28 pixel canvas. Here are some examples of how the images can look like:\n",
    "\n",
    "![The first handwritten digits in the MNIST training data](mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will read the training data and the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the training data matrices: (60000, 784) (60000, 10)\n",
      "Shapes of the test data matrices: (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import mnist_network\n",
    "\n",
    "training_x, training_y = mnist_network.read_training_data()\n",
    "print('Shapes of the training data matrices: {} {}'.format(training_x.shape, training_y.shape))\n",
    "\n",
    "test_x, test_y = mnist_network.read_test_data()\n",
    "print('Shapes of the test data matrices: {} {}'.format(test_x.shape, test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a Python perspective, each of the two data sets is a pair `(x, y)` of matrices: Each row of `x` is a 784-component vector containing the greyscale values of the pixels in an image as floats between 0 and 1. Each row of `y` is a 10-component one-hot vector representing the digit corresponding to the image. As an example, here is the vector for the first digit in the test data, the digit 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to implement a deep feedforward network that reads a batch of image vectors and predicts the corresponding digits. Your network should conform to the following specification:\n",
    "\n",
    "* one input layer, one output layer, flexible number of hidden layers\n",
    "* activation function for hidden layers: rectified linear unit (ReLU)\n",
    "* activation function for output layer: softmax\n",
    "* error function for gradient computation: categorical cross-entropy\n",
    "\n",
    "To get you started on this task, we provide skeleton code and a number of useful helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "The following cell contains NumPy-based implementations of the ReLU activation function and its derivative (which you should use for the hidden layers of your network), as well as the softmax activation function (for the output layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each case, the argument `x` is a batch of input values, such as `training_x`. The implementation of the softmax function uses a standard trick to improve numerical stability; see [this link](http://stackoverflow.com/questions/34968722/softmax-function-python) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skeleton code\n",
    "\n",
    "To get you started, we provide the following skeleton code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, sizes):\n",
    "        self.layer_dims = sizes\n",
    "#       self.ws = [np.zeros((m, n)) for m, n in zip(self.sizes[1:],self.sizes[:-1])]\n",
    "#       self.bs = [np.zeros((n, 1)) for n in self.sizes[1:]]\n",
    "        self.L = len(self.layer_dims)\n",
    "        self.parameters = {}\n",
    "        for l in range(1, self.L):\n",
    "            self.parameters['W' + str(l)] = 0.001*np.random.randn(self.layer_dims[l],self.layer_dims[l-1])\n",
    "            self.parameters['b' + str(l)] = 0.001*np.random.randn(self.layer_dims[l],1)    \n",
    "        \n",
    "    def relu(self,x):\n",
    "        cache = (x)\n",
    "        return x * (x > 0), cache\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        e_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "        cache = (x)\n",
    "        return e_x / e_x.sum(axis=0, keepdims=True), cache\n",
    "    \n",
    "    def relu_prime(self,x):\n",
    "        return 1 * (x > 0)\n",
    "    \n",
    "    \n",
    "    def linear_forward(self,A, W, b):\n",
    "        Z = np.dot(W,A) + b    \n",
    "        cache = (A, W, b)\n",
    "        return Z, cache\n",
    "    \n",
    "    def forward(self,x):\n",
    "        caches = []\n",
    "        A = x.T\n",
    "        LL =  len(self.parameters)//2\n",
    "        for l in range(1, LL):\n",
    "            A_prev = A\n",
    "            Z, linear_cache = self.linear_forward(A_prev, self.parameters['W' + str(l)], self.parameters['b' + str(l)])\n",
    "            A, activation_cache = self.relu(Z)\n",
    "            cache  = (linear_cache,activation_cache)\n",
    "            caches.append(cache)\n",
    "            \n",
    "        ZL, linear_cache = self.linear_forward(A, self.parameters['W' + str(LL)], self.parameters['b' + str(LL)])\n",
    "        AL, activation_cache = self.softmax(ZL) \n",
    "        cache  = (linear_cache,activation_cache)\n",
    "        caches.append(cache)\n",
    "        return AL, caches\n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "        yy,caches = self.forward(x)\n",
    "        yhat = np.argmax(yy, axis=0)\n",
    "        return yhat\n",
    "    \n",
    "\n",
    "    def linear_backward(self,dZ, cache):\n",
    "        A_prev, W, b = cache\n",
    "        m = A_prev.shape[1]\n",
    "        dW = 1/m*(np.dot(dZ,A_prev.T))\n",
    "        db = 1/m*(np.sum(dZ,axis = 1,keepdims = True))\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "\n",
    "    def linear_activation_backward(self,dA, cache):\n",
    "        linear_cache, activation_cache = cache\n",
    "        dZ = dA*self.relu_prime(activation_cache)\n",
    "        dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "    def backpropagate(self, AL, y, caches):\n",
    "        grads = {}\n",
    "        L = len(caches) \n",
    "        m = AL.shape[1]\n",
    "        Y = y.T\n",
    "        dZL = (AL - Y)\n",
    "        current_cache = caches[L-1]\n",
    "        ALprev, WL, bL = current_cache[0]\n",
    "        grads[\"dW\" + str(L)] = 1/m*(np.dot(dZL,ALprev.T))\n",
    "        grads[\"db\" + str(L)]  = 1/m*(np.sum(dZL, axis=1, keepdims=True))\n",
    "        grads[\"dA\" + str(L-1)] = np.dot(WL.T,dZL)\n",
    "        \n",
    "        for l in reversed(range(L-1)):\n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = self.linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache)\n",
    "            grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "            grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "            grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def update_parameters(self, grads, learning_rate):\n",
    "        L = len(self.parameters) // 2 \n",
    "        for l in range(L):\n",
    "            self.parameters[\"W\" + str(l+1)] = self.parameters[\"W\" + str(l+1)] - learning_rate *grads[\"dW\"+str(l+1)]\n",
    "            self.parameters[\"b\" + str(l+1)] = self.parameters[\"b\" + str(l+1)] - learning_rate *grads[\"db\"+str(l+1)]\n",
    "        return self.parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a class Network that represents deep feedforward networks. The intended behaviour of the fields and methods of this class is specified below. In the skeleton code, all fields are initialized with, and all methods return, zero matrices of the appropriate shape. In order to obtain a network that meets the requirements, you will have to write code that replaces these placeholders with meaningful values.\n",
    "\n",
    "In your implementation, you may choose to add more fields and/or methods than the ones included in the skeleton code. However, in all of your code, you may only call functions from the NumPy library, but no other library.\n",
    "\n",
    "### Fields\n",
    "\n",
    "**sizes : list(int)**\n",
    "\n",
    "The dimensions of the network layers, from the first (input) layer to the last (output) layer.\n",
    "\n",
    "An example, in a network with 784 units in the input layer, 10 units in the output layer, and 100 units in the (single) hidden layer this field would have the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[784, 700, 100, 10]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[784, 700,100, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ws : list(np.array)**\n",
    "\n",
    "The weight matrices of the network, where the matrix at index $i$ holds the weights of the connections from layer $i$ to layer $i+1$. As an example, if the width of layer&nbsp;0 is $784$ and the width of layer&nbsp;1 is $100$, then `ws[0]` will have shape $(784, 100)$.\n",
    "\n",
    "**bs : list(np.array)**\n",
    "\n",
    "The bias vectors of the network, where the vector at index $i$ holds the biases for layer $i+1$. As an example, `bs[0]` is the bias vector of layer&nbsp;1. Note that there are no biases for the input layer (layer&nbsp;0).\n",
    "\n",
    "### Initialization\n",
    "\n",
    "Initialize the weights and biases of the network. Note that in the starter code, both weights and biases are initialized using zeros.\n",
    "\n",
    "**sizes : list(int)**\n",
    "\n",
    "The dimensions of the network layers, from the first (input) layer to the last (output) layer.\n",
    "\n",
    "As an example, the following code creates a network with 784 units in the input layer, 10 units in the output layer, and 100 units in the (single) hidden layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forward\n",
    "\n",
    "Computes the output of the network for a batch of input values.\n",
    "\n",
    "**x : np.array**\n",
    "\n",
    "A batch of input values, such as `training_x`.\n",
    "\n",
    "**Returns:** The output of the network for the specified input. This will be an array of shape $(m, n)$ where $m$ is the number of rows in the input batch, and $n$ is the size of the last layer in the network. In the starter code, the method returns an array of all zeros.\n",
    "\n",
    "### predict\n",
    "\n",
    "Predicts the digits for a batch of input values.\n",
    "\n",
    "**x : np.array**\n",
    "\n",
    "A batch of input values, such as `test_x`.\n",
    "\n",
    "**Returns:** The digits predicted for the specified input. This will be an array of shape $(m, 1)$ where $m$ is the number of rows in the input batch $x$. In the starter code, the method returns an array of all zeros.\n",
    "\n",
    "### backpropagate\n",
    "\n",
    "Computes the network gradients for a batch of input and corresponding output values. In this assignment, the gradients should be computed based on categorical cross-entropy as the error function.\n",
    "\n",
    "During backpropagation, you will have to compute the derivative of the cross-entropy error function with respect to the softmax *input*. While you could do that by multiplying the derivative of the cross-entropy error function with respect to the softmax *output* and the derivative of the softmax function with respect to its input, that particular product turns out to have a very simple form. See [this page](http://peterroelants.github.io/posts/cross_entropy_softmax/) for a derivation.\n",
    "\n",
    "**x : np.array**\n",
    "\n",
    "A batch of input values, such as `training_x`.\n",
    "\n",
    "**y : np.array**\n",
    "\n",
    "A batch of corresponding output values, such as `training_y`.\n",
    "\n",
    "**Returns:** A list of pairs of the form $(\\nabla w, \\nabla b)$, one for each non-input layer of the network, where the first component of each pair is the average gradient for the weights of the connections coming into the layer and the second component is the average gradient for the biases at the layer. In the starter code, the method returns a list of zero gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Train your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed the Network class, your second task is to write code to train the network using stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "The function in the next code cell will sample minibatches from an array `x` of input values and a corresponding array `y` of output values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatches(x, y, batch_size):\n",
    "    random_indices = np.random.permutation(np.arange(x.shape[0]))\n",
    "    for i in range(0, x.shape[0] - batch_size + 1, batch_size):\n",
    "        batch_indices = random_indices[i:i+batch_size]\n",
    "        yield x[batch_indices], y[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function computes the test error rate of a network on a batch of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net):\n",
    "    return np.mean(net.predict(test_x) != np.argmax(test_y, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skeleton code\n",
    "\n",
    "The following cell contains skeleton code for the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd(net, x, y, n_epochs, batch_size, eta=0.16):\n",
    "    for t in range(n_epochs):\n",
    "        for batch_x, batch_y in minibatches(x, y, batch_size):\n",
    "            ALL,caches = net.forward(batch_x)\n",
    "            grads =  net.backpropagate(ALL,batch_y,caches)\n",
    "            parameters = net.update_parameters(grads, eta/((t+1.0)))\n",
    "                    \n",
    "        print(\"epoch = {}, test error rate = {:.4f}\".format(t, evaluate(net)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intended meaning of the various parameters is as follows:\n",
    "\n",
    "**x : np.array**\n",
    "\n",
    "A batch of input values, such as `training_x`.\n",
    "\n",
    "**y : np.array**\n",
    "\n",
    "A batch of corresponding output values, such as `training_y`.\n",
    "\n",
    "**n_epochs : int**\n",
    "\n",
    "The number of iterations over the training data (&lsquo;epochs&rsquo;).\n",
    "\n",
    "**batch_size : int**\n",
    "\n",
    "The number of input values per minibatch.\n",
    "\n",
    "**eta : float**\n",
    "\n",
    "The learning rate in the stochastic gradient descent update step.\n",
    "\n",
    "### Intended usage\n",
    "\n",
    "To see how the training code is intended to be used, here is how you set up a network and train it on the training data for 2&nbsp;iterations with minibatch size&nbsp;30 and the default learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, test error rate = 0.0351\n",
      "epoch = 1, test error rate = 0.0251\n",
      "epoch = 2, test error rate = 0.0230\n",
      "epoch = 3, test error rate = 0.0204\n",
      "epoch = 4, test error rate = 0.0203\n",
      "epoch = 5, test error rate = 0.0195\n",
      "epoch = 6, test error rate = 0.0199\n",
      "epoch = 7, test error rate = 0.0187\n",
      "epoch = 8, test error rate = 0.0197\n",
      "epoch = 9, test error rate = 0.0188\n"
     ]
    }
   ],
   "source": [
    "net = Network([784,200,10])\n",
    "train_sgd(net, training_x, training_y, 10,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance goal\n",
    "\n",
    "Once you have a working network and training algorithm, you can compare the error rate of your network to the results on the [MNIST website](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "**To get credit for this assignment, your network must achieve a test error rate of less than 2% at least once during the first 10 epochs of training.**\n",
    "\n",
    "To tune your network, you can play around with the various training parameters: number of epochs, minibatch size, and learning rate. In addition to that, you can also make more substantial changes such as the following:\n",
    "\n",
    "* Make the network wider (increase the size of a layer) or deeper (add more layers).\n",
    "* Implement a different initialization strategy.\n",
    "* Implement a regularization method or dropout.\n",
    "* Implement an optimization algorithm with an adaptive learning rate, such as RMSProp or Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have reached the performance goal, send this notebook to Marco to receive credit for the assignment. The notebook must be self-contained and must run without error.\n",
    "\n",
    "In addition to your code, you are asked to submit a short text (less than 500&nbsp;words) in which you reflect on what you have done. Which specific choices did you make when tuning your network? How did these choices affect performance? You can enter your text in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report\n",
    "\n",
    "I would like to mention that I have done this lab by reusing some on my own material from project work of another course which I have done it in Coursera website. The course was part of Deep Learning Spacialization and it was the first course (Neural Networks and Deep Learning) in that specialization. \n",
    "\n",
    "I have used the vectorized implementation from there to implement the forward and backward propagation part of this lab. \n",
    "\n",
    "In my implementation I choose to have one hidden layer which has 200 nodes. I have tried different number of nodes and finaly I get to this number. \n",
    "\n",
    "In addition, during my trial and error, I started with zero initial matrices W and biases b but I did not get desired results. Then I used gaussian random initial values for both W and b in addition I multiplied it with different prefactor until I got close to the desired error but prefactor 0.001. \n",
    "\n",
    "I have done searching for different learning rate and saw the effect that with larger values we have the zigzag behaviour and smaller values are not converging fast. I got my desired error rate by using learning rate 0.16.  \n",
    "\n",
    "For the minibach size I search for large values around 100-300 in which I did not get the error rate below 2 percent. We I change it to smaller values I saw some promising reaction so I lower them to 10 and I got the result.\n",
    "\n",
    "I was thinking that adding more layer will help to improve the performance but it was not what I observed when I added more hidden layers. For example if I add a layer with 50 nodes after my first hidden layer, the performance drops. \n",
    "\n",
    "This parameter tuning task seems to heuristic to me and I still need to get more about it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
